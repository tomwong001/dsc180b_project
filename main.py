# -*- coding: utf-8 -*-
"""2D_iclr2022_SIR_NP_LIG_heldout.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uBHJpKv6m3l356qoyajxyf2bRBM9VETY
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import torch
import matplotlib.pyplot as plt
# %matplotlib inline
import torch.nn as nn
import time
import argparse
import os
from config import cfg 
from src.seir import seir
from src.dcrnn import DCRNNModel


# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

device = torch.device("cpu")
large = 25; med = 19; small = 12
params = {'axes.titlesize': large,
          'legend.fontsize': 20,
          'figure.figsize': (27, 8),
          'axes.labelsize': med,
          'xtick.labelsize': med,
          'ytick.labelsize': med,
          'figure.titlesize': med}
plt.rcParams.update(params)




def MAE(pred, target):
    loss = torch.abs(pred-target)
    return loss.mean()

def random_split_context_target(x,y, n_context):
    """Helper function to split randomly into context and target"""
    ind = np.arange(x.shape[0])
    mask = np.random.choice(ind, size=n_context, replace=False)
    return x[mask], y[mask], np.delete(x, mask, axis=0), np.delete(y, mask, axis=0)

def sample_z(mu, logvar,z_dim, n=1):
    """Reparameterisation trick."""
    if n == 1:
        eps = torch.autograd.Variable(logvar.data.new(z_dim).normal_())
    else:
        eps = torch.autograd.Variable(logvar.data.new(n,z_dim).normal_())
    
    std = 0.1+ 0.9*torch.sigmoid(logvar)
    return mu + std * eps

def data_to_z_params(dcrnn, x, y):
    """Helper to batch together some steps of the process."""
    xy = torch.cat([x,y], dim=1)
    rs = dcrnn.repr_encoder(xy)
    r_agg = rs.mean(dim=0) # Average over samples
    return dcrnn.z_encoder(r_agg) # Get mean and variance for q(z|...)

def test(dcrnn, x_train, y_train, x_test, z_dim):
    with torch.no_grad():
      z_mu, z_logvar = data_to_z_params(dcrnn, x_train.to(device),y_train.to(device))
      
      output_list = []
      for i in range (len(x_test)):
          zsamples = sample_z(z_mu, z_logvar, z_dim) 
          output = dcrnn.decoder(x_test[i:i+1].to(device), zsamples).cpu()
          output_list.append(output.detach().numpy())
    
    return np.concatenate(output_list)

def train(dcrnn, opt, n_epochs, x_train, y_train, x_val, y_val, x_test, y_test, n_display=500, patience = 5000): #7000, 1000
    train_losses = []
    # mae_losses = []
    # kld_losses = []
    val_losses = []
    test_losses = []

    means_test = []
    stds_test = []
    N = 100000 #population
    min_loss = 0. # for early stopping
    wait = 0
    min_loss = float('inf')
    
    for t in range(n_epochs): 
        opt.zero_grad()
        #Generate data and process
        x_context, y_context, x_target, y_target = random_split_context_target(
                                x_train, y_train, int(len(y_train)*0.1)) #0.25, 0.5, 0.05,0.015, 0.01
        # print(x_context.shape, y_context.shape, x_target.shape, y_target.shape)    

        x_c = torch.from_numpy(x_context).float().to(device)
        x_t = torch.from_numpy(x_target).float().to(device)
        y_c = torch.from_numpy(y_context).float().to(device)
        y_t = torch.from_numpy(y_target).float().to(device)

        x_ct = torch.cat([x_c, x_t], dim=0).float().to(device)
        y_ct = torch.cat([y_c, y_t], dim=0).float().to(device)

        y_pred = dcrnn(x_t, x_c, y_c, x_ct, y_ct)

        train_loss = N * MAE(y_pred, y_t)/100 + dcrnn.KLD_gaussian()
        mae_loss = N * MAE(y_pred, y_t)/100
        kld_loss = dcrnn.KLD_gaussian()
        
        train_loss.backward()
        torch.nn.utils.clip_grad_norm_(dcrnn.parameters(), 5) #10
        opt.step()
        
        #val loss
        y_val_pred = test(dcrnn, torch.from_numpy(x_train).float(),torch.from_numpy(y_train).float(),
                      torch.from_numpy(x_val).float(), cfg.MODEL.z_dim)
        val_loss = N * MAE(torch.from_numpy(y_val_pred).float(),torch.from_numpy(y_val).float())/100
        #test loss
        y_test_pred = test(dcrnn, torch.from_numpy(x_train).float(),torch.from_numpy(y_train).float(),
                      torch.from_numpy(x_test).float(), cfg.MODEL.z_dim)
        test_loss = N * MAE(torch.from_numpy(y_test_pred).float(),torch.from_numpy(y_test).float())/100
        if t % n_display ==0:
            print('train loss:', train_loss.item(), 'mae:', mae_loss.item(), 'kld:', kld_loss.item())
            print('val loss:', val_loss.item(), 'test loss:', test_loss.item())

        if t % (n_display/10) ==0:
            train_losses.append(train_loss.item())
            val_losses.append(val_loss.item())
            test_losses.append(test_loss.item())
            # mae_losses.append(mae_loss.item())
            # kld_losses.append(kld_loss.item())

        #early stopping
        if val_loss < min_loss:
            wait = 0
            min_loss = val_loss
            
        elif val_loss >= min_loss:
            wait += 1
            if wait == patience:
                print('Early stopping at epoch: %d' % t)
                return train_losses, val_losses, test_losses, dcrnn.z_mu_all, dcrnn.z_logvar_all
        
    return train_losses, val_losses, test_losses, dcrnn.z_mu_all, dcrnn.z_logvar_all

def select_data(cfg, x_train, y_train, beta_epsilon_all, yall_set, score_array, selected_mask):

    mask_score_array = score_array*(1-selected_mask)
    # print('mask_score_array',mask_score_array)
    select_index = np.argmax(mask_score_array)
    print('select_index:',select_index)


    selected_x = beta_epsilon_all[select_index:select_index+1]
    selected_y = yall_set[select_index]

    x_train1 = np.repeat(selected_x,cfg.SIMULATOR.num_simulations,axis =0)
    x_train = np.concatenate([x_train, x_train1],0)
    
    y_train1 = selected_y.reshape(-1,100)
    y_train = np.concatenate([y_train, y_train1],0)
 
    selected_mask[select_index] = 1
    
    return x_train, y_train, selected_mask

def calculate_score(cfg, dcrnn, x_train, y_train, beta_epsilon_all):
    x_train = torch.from_numpy(x_train).float()
    y_train = torch.from_numpy(y_train).float()

    # query z_mu, z_var of the current training data
    with torch.no_grad():
        z_mu, z_logvar = data_to_z_params(x_train.to(device),y_train.to(device))

        score_list = []
        for i in range(len(beta_epsilon_all)):
            # generate x_search
            x1 = beta_epsilon_all[i:i+1]
            x_search = np.repeat(x1,cfg.SIMULATOR.num_simulations,axis =0)
            x_search = torch.from_numpy(x_search).float()

            # generate y_search based on z_mu, z_var of current training data
            output_list = []
            for j in range (len(x_search)):
                zsamples = sample_z(z_mu, z_logvar,cfg.MODEL.z_dim) 
                output = dcrnn.decoder(x_search[j:j+1].to(device), zsamples).cpu()
                output_list.append(output.detach().numpy())

            y_search = np.concatenate(output_list)
            y_search = torch.from_numpy(y_search).float()

            x_search_all = torch.cat([x_train,x_search],dim=0)
            y_search_all = torch.cat([y_train,y_search],dim=0)

            # generate z_mu_search, z_var_search
            z_mu_search, z_logvar_search = data_to_z_params(x_search_all.to(device),y_search_all.to(device))
            
            # calculate and save kld
            mu_q, var_q, mu_p, var_p = z_mu_search,  0.1+ 0.9*torch.sigmoid(z_logvar_search), z_mu, 0.1+ 0.9*torch.sigmoid(z_logvar)

            std_q = torch.sqrt(var_q)
            std_p = torch.sqrt(var_p)

            p = torch.distributions.Normal(mu_p, std_p)
            q = torch.distributions.Normal(mu_q, std_q)
            score = torch.distributions.kl_divergence(p, q).sum()

            score_list.append(score.item())

        score_array = np.array(score_list)

    return score_array

"""BO search:"""

def mae_plot(mae, selected_mask,i,j):
    epsilon, beta  = np.meshgrid(np.linspace(0.25, 0.7, 10), np.linspace(1.1, 4.1, 31))
    selected_mask = selected_mask.reshape(30,9)
    mae_min, mae_max = 0, 1200

    fig, ax = plt.subplots(figsize=(16, 7))
    # f, (y1_ax) = plt.subplots(1, 1, figsize=(16, 10))

    c = ax.pcolormesh(beta-0.05, epsilon-0.025, mae, cmap='binary', vmin=mae_min, vmax=mae_max)
    ax.set_title('MAE Mesh')
    # set the limits of the plot to the limits of the data
    ax.axis([beta.min()-0.05, beta.max()-0.05, epsilon.min()-0.025, epsilon.max()-0.025])
    x,y = np.where(selected_mask==1)
    x = x*0.1+1.1
    y = y*0.05+0.25
    ax.plot(x, y, 'r*', markersize=15)
    fig.colorbar(c, ax=ax)
    ax.set_xlabel('Beta')
    ax.set_ylabel('Epsilon')
    plt.savefig('mae_plot_seed%d_itr%d.pdf' % (i,j))

def score_plot(score, selected_mask,i,j):
    epsilon, beta  = np.meshgrid(np.linspace(0.25, 0.7, 10), np.linspace(1.1, 4.1, 31))
    score_min, score_max = 0, 1
    selected_mask = selected_mask.reshape(30,9)
    score = score.reshape(30,9)
    fig, ax = plt.subplots(figsize=(16, 7))
    # f, (y1_ax) = plt.subplots(1, 1, figsize=(16, 10))

    c = ax.pcolormesh(beta-0.05, epsilon-0.025, score, cmap='binary', vmin=score_min, vmax=score_max)
    ax.set_title('Score Mesh')
    # set the limits of the plot to the limits of the data
    ax.axis([beta.min()-0.05, beta.max()-0.05, epsilon.min()-0.025, epsilon.max()-0.025])
    x,y = np.where(selected_mask==1)
    x = x*0.1+1.1
    y = y*0.05+0.25
    ax.plot(x, y, 'r*', markersize=15)
    fig.colorbar(c, ax=ax)
    ax.set_xlabel('Beta')
    ax.set_ylabel('Epsilon')
    plt.savefig('score_plot_seed%d_itr%d.pdf' % (i,j))

def MAE_MX(y_pred, y_test):
    N = 100000
    y_pred = y_pred.reshape(30,9, 30, 100)*N/100
    y_test = y_test.reshape(30,9, 30, 100)*N/100
    mae_matrix = np.mean(np.abs(y_pred - y_test),axis=(2,3))
    mae = np.mean(np.abs(y_pred - y_test))
    return mae_matrix, mae

def main(args):
    num_days = cfg.SIMULATOR.num_days
    num_simulations = cfg.SIMULATOR.num_simulations

    [b_low, b_high, b_step], [e_low, e_high, e_step] = cfg.SIMULATOR.train_param
    beta = np.repeat(np.expand_dims(np.linspace(b_low, b_high, b_step),1),e_step,1) #1.1, 4.0, 30
    epsilon = np.repeat(np.expand_dims(np.linspace(e_low, e_high, e_step),0),b_step,0) #0.25, 0.65, 9
    beta_epsilon = np.stack([beta,epsilon],-1)
    beta_epsilon_train = beta_epsilon.reshape(-1,2)

    [b_low, b_high, b_step], [e_low, e_high, e_step] = cfg.SIMULATOR.val_param
    beta = np.repeat(np.expand_dims(np.linspace(b_low, b_high, b_step),1),e_step,1) #1.14, 3.88, 5
    epsilon = np.repeat(np.expand_dims(np.linspace(e_low, e_high, e_step),0),b_step,0) #0.29, 0.59, 3
    beta_epsilon = np.stack([beta,epsilon],-1)
    beta_epsilon_val = beta_epsilon.reshape(-1,2)

    [b_low, b_high, b_step], [e_low, e_high, e_step] = cfg.SIMULATOR.test_param
    beta = np.repeat(np.expand_dims(np.linspace(b_low, b_high, b_step),1),e_step,1) #1.24, 3.98, 5
    epsilon = np.repeat(np.expand_dims(np.linspace(e_low, e_high, e_step),0),b_step,0) #0.31, 0.61, 3
    beta_epsilon = np.stack([beta,epsilon],-1)
    beta_epsilon_test = beta_epsilon.reshape(-1,2)

    beta_epsilon_all = beta_epsilon_train
    yall_set, yall_mean, yall_std = seir(num_days,beta_epsilon_all,num_simulations, cfg)
    y_all = yall_set.reshape(-1,num_days-1)
    x_all = np.repeat(beta_epsilon_all,num_simulations,axis =0)

    yval_set, yval_mean, yval_std = seir(num_days,beta_epsilon_val,num_simulations, cfg)
    y_val = yval_set.reshape(-1,num_days-1)
    x_val = np.repeat(beta_epsilon_val,num_simulations,axis =0)


    ytest_set, ytest_mean, ytest_std = seir(num_days,beta_epsilon_test,num_simulations, cfg)
    y_test = ytest_set.reshape(-1, num_days-1)
    x_test = np.repeat(beta_epsilon_test,num_simulations,axis =0)

    np.random.seed(3)
    mask_init = np.zeros(len(beta_epsilon_all))
    mask_init[:2] = 1

    np.random.shuffle(mask_init)
    selected_beta_epsilon = beta_epsilon_all[mask_init.astype('bool')]
    x_train_init = np.repeat(selected_beta_epsilon,num_simulations,axis =0)

    selected_y = yall_set[mask_init.astype('bool')]
    y_train_init = selected_y.reshape(selected_y.shape[0]*selected_y.shape[1],selected_y.shape[2])

    r_dim = cfg.MODEL.r_dim
    z_dim = cfg.MODEL.z_dim #8
    x_dim = cfg.MODEL.x_dim #
    y_dim = cfg.MODEL.y_dim #50
    N = cfg.SIMULATOR.population #population

    ypred_allset = []
    ypred_testset = []
    mae_allset = []
    maemetrix_allset = []
    mae_testset = []
    score_set = []
    mask_set = []

    for seed in range(1,3): #3
        np.random.seed(seed)
        dcrnn = DCRNNModel(x_dim, y_dim, r_dim, z_dim, device=device).to(device)
        opt = torch.optim.Adam(dcrnn.parameters(), cfg.MODEL.lr) #1e-3

        y_pred_test_list = []
        y_pred_all_list = []
        all_mae_matrix_list = []
        all_mae_list = []
        test_mae_list = []
        score_list = []
        mask_list = []

        x_train,y_train = x_train_init, y_train_init
        # selected_mask = init()
        selected_mask = np.copy(mask_init)
        #import ipdb;ipdb.set_trace()
        
        for i in range(cfg.TRAIN.train_iter): #8
            # print('selected_mask:', selected_mask)
            print('i = {}, x_train shape = {}, y_train shape = {}, '.format(i, x_train.shape, y_train.shape))
            mask_list.append(np.copy(selected_mask))

            start_time = time.time()
            train_losses, val_losses, test_losses, z_mu, z_logvar = train(
                dcrnn, opt, cfg.TRAIN.stnp_epoch ,x_train,
                y_train,x_val, y_val, x_test, y_test,cfg.TRAIN.n_display, cfg.TRAIN.patience,
            ) #20000, 5000
            end_time = time.time()
            print('train time = {}'.format(end_time - start_time))

            start_time = time.time()
            y_pred_test = test(torch.from_numpy(x_train).float(),torch.from_numpy(y_train).float(),
                            torch.from_numpy(x_test).float())
            end_time = time.time()
            print('test time on train set = {}'.format(end_time - start_time))
            y_pred_test_list.append(y_pred_test)

            test_mae = N * MAE(torch.from_numpy(y_pred_test).float(),torch.from_numpy(y_test).float())/100
            test_mae_list.append(test_mae.item())
            print('Test MAE:',test_mae.item())

            start_time = time.time()
            y_pred_all = test(torch.from_numpy(x_train).float(),torch.from_numpy(y_train).float(),
                            torch.from_numpy(x_all).float())
            y_pred_all_list.append(y_pred_all)
            mae_matrix, mae = MAE_MX(y_pred_all, y_all)
            end_time = time.time()
            print('test time on all = {}'.format(end_time - start_time))
            
            
            all_mae_matrix_list.append(mae_matrix)
            all_mae_list.append(mae)
            print('All MAE:',mae)
            mae_plot(mae_matrix, selected_mask,seed,i)

            start_time = time.time()
            score_array = calculate_score(cfg, dcrnn, x_train, y_train, beta_epsilon_all)
            score_array = (score_array - np.min(score_array))/(np.max(score_array) - np.min(score_array))
            end_time = time.time()
            print('score calculation = {}'.format(end_time - start_time))
            
            score_list.append(score_array)
            score_plot(score_array, selected_mask,seed,i)

            start_time = time.time()
            x_train, y_train, selected_mask = select_data(x_train, y_train, beta_epsilon_all, yall_set, score_array, selected_mask)
            end_time = time.time()
            print('data selection = {}'.format(end_time - start_time))

        y_pred_all_arr = np.stack(y_pred_all_list,0)
        y_pred_test_arr = np.stack(y_pred_test_list,0)
        all_mae_matrix_arr = np.stack(all_mae_matrix_list,0)
        all_mae_arr = np.stack(all_mae_list,0)
        test_mae_arr = np.stack(test_mae_list,0)
        score_arr = np.stack(score_list,0)
        mask_arr = np.stack(mask_list,0)

        ypred_allset.append(y_pred_all_arr)
        ypred_testset.append(y_pred_test_arr)
        maemetrix_allset.append(all_mae_matrix_arr)
        mae_allset.append(all_mae_arr)
        mae_testset.append(test_mae_arr)
        score_set.append(score_arr)
        mask_set.append(mask_arr)

    ypred_allarr = np.stack(ypred_allset,0)
    ypred_testarr = np.stack(ypred_testset,0) 
    maemetrix_allarr = np.stack(maemetrix_allset,0) 
    mae_allarr = np.stack(mae_allset,0)
    mae_testarr = np.stack(mae_testset,0)
    score_arr = np.stack(score_set,0)
    mask_arr = np.stack(mask_set,0)

    # np.save('mae_testarr.npy',mae_testarr)
    # np.save('mae_allarr.npy',mae_allarr)
    # np.save('maemetrix_allarr.npy',maemetrix_allarr)

    # np.save('score_arr.npy',score_arr)
    # np.save('mask_arr.npy',mask_arr)

    # np.save('y_pred_all_arr.npy',ypred_allarr)
    # np.save('y_pred_test_arr.npy',ypred_testarr)

    # np.save('y_all.npy',y_all)
    # np.save('y_test.npy',y_test)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="STNP network with RL"
    )
    parser.add_argument(
        "--cfg",
        default="config/config.yaml",
        metavar="FILE",
        help="path to config file",
        type=str,
    )

    args = parser.parse_args()
    print(f'args cfg = {args.cfg}')
    cfg.merge_from_file(args.cfg)
    
    cfg.DIR.output_dir = os.path.join(cfg.DIR.snapshot, cfg.DIR.exp)
    if not os.path.exists(cfg.DIR.output_dir):
        os.mkdir(cfg.DIR.output_dir)    

    cfg.TRAIN.resume = os.path.join(cfg.DIR.output_dir, cfg.TRAIN.resume)
    cfg.VAL.resume = os.path.join(cfg.DIR.output_dir, cfg.VAL.resume)

    with open(os.path.join(cfg.DIR.output_dir, 'config.yaml'), 'w') as f:
        f.write("{}".format(cfg))

    main(cfg)

